{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "# https://github.com/Ujjwal-9/Knowledge-Distillation\n",
    "sys.path.append('Knowledge/utils/')\n",
    "import sklearn\n",
    "import os\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "import os\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "# use non standard flow_from_directory\n",
    "# it outputs y_batch that contains onehot targets and logits\n",
    "from image_preprocessing_ver2 import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logits saved from teacher\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "train_logits = np.load(os.path.join(data_dir, 'cifar10_train_logits.npy'), allow_pickle=True)[()]\n",
    "val_logits = np.load(os.path.join(data_dir, 'cifar10_val_logits.npy'), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar10 images and associated teacher logits into ImageDataGenerator for batch processing\n",
    "\n",
    "data_generator = ImageDataGenerator(data_format='channels_last', rescale=1/255)\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = r''\n",
    "train_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'cifar10\\\\train'), train_logits, target_size=(32, 32), color_mode='rgb', batch_size=batch_size)\n",
    "val_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'cifar10\\\\test'), val_logits, target_size=(32, 32), color_mode='rgb', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define student model\n",
    "\n",
    "from keras import models, layers\n",
    "\n",
    "def load_student_for_training():\n",
    "    from keras.initializers import lecun_normal\n",
    "    from keras.layers import BatchNormalization\n",
    "    \n",
    "    initializer = lecun_normal(seed=1)\n",
    "    \n",
    "    student = Sequential()\n",
    "    student.add(Conv2D(16, (3, 3),\n",
    "                     input_shape=(32, 32, 3),\n",
    "                      kernel_initializer=initializer))\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    student.add(Conv2D(32, (3, 3),\n",
    "                       kernel_initializer=initializer))\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    student.add(Flatten())\n",
    "    student.add(Dense(64, kernel_initializer=initializer))\n",
    "    student.add(BatchNormalization())\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(Dense(10, kernel_initializer=initializer))\n",
    "    student.add(Activation('softmax'))\n",
    "    \n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss (soft targets and hard targets)\n",
    "\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras import backend as K\n",
    "\n",
    "def distillation_loss(y_true, y_pred, hard_loss_weight, temp):\n",
    "    y_true, logits = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    y_soft = K.softmax(logits / temp)\n",
    "    \n",
    "    y_pred, y_pred_soft = y_pred[:, :10], y_pred[:, 10:]\n",
    "    \n",
    "    return hard_loss_weight * logloss(y_true, y_pred) + logloss(y_soft, y_pred_soft)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric functions\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "def top_5_accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return top_k_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def soft_logloss(y_true, y_pred, temp):     \n",
    "    logits = y_true[:, 10:]\n",
    "    y_soft = K.softmax(logits/temp)\n",
    "    y_pred_soft = y_pred[:, 10:]    \n",
    "    return logloss(y_soft, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(temp, hard_weight, epochs=25, verbose=False):\n",
    "    \"\"\"\n",
    "    Metrics are redefined here because soft_logloss depends on non-standard param (temp).\n",
    "    model.compile wouldn't take lambdas as metrics, so this was the workaround.\n",
    "    \"\"\"\n",
    "    def accuracy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "    def top_5_accuracy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return top_k_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "    def categorical_crossentropy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return logloss(y_true, y_pred)\n",
    "\n",
    "    def soft_logloss(y_true, y_pred):     \n",
    "        logits = y_true[:, 10:]\n",
    "        y_soft = K.softmax(logits/temp)\n",
    "        y_pred_soft = y_pred[:, 10:]    \n",
    "        return logloss(y_soft, y_pred_soft)\n",
    "    \n",
    "    student = load_student_for_training()\n",
    "    \n",
    "    # Remove softmax\n",
    "    student.pop()\n",
    "    \n",
    "    # Get student logits and class probabilities\n",
    "    logits = student.layers[-1].output\n",
    "    probabilities = layers.Activation('softmax')(logits)\n",
    "\n",
    "    # Apply temperature to get softed probabilities\n",
    "    # Temps of 2.5-4 \"worked significantly better\" than other temps on networks with 30 units per layer\n",
    "    logits_T = layers.Lambda(lambda x: x / temp)(logits)\n",
    "    probabilities_T = layers.Activation('softmax')(logits_T)\n",
    "\n",
    "    # Define student that outputs probabilities and softed probabilities\n",
    "    output = layers.concatenate([probabilities, probabilities_T])\n",
    "    model = Model(student.input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, hard_weight, temp),\n",
    "        metrics=[accuracy, top_5_accuracy, categorical_crossentropy, soft_logloss])\n",
    "\n",
    "    if verbose:\n",
    "        verbose = 1\n",
    "    else:\n",
    "        verbose = 0\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=50000/batch_size,\n",
    "        verbose=verbose,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=25,\n",
    "        callbacks=[\n",
    "                EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.005)\n",
    "            ])\n",
    "\n",
    "    results = model.evaluate_generator(val_generator_no_shuffle, steps=50000/batch_size)\n",
    "    return results, history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define val data generator\n",
    "\n",
    "val_generator_no_shuffle = data_generator.flow_from_directory(\n",
    "    os.path.join(data_dir, 'cifar10\\\\test'), val_logits,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=128, color_mode='rgb', shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over param combinations, saving results of each training session\n",
    "\n",
    "temp_accuracies_weight_1 = []\n",
    "for temp in [1, 2, 5, 10]:\n",
    "    try:\n",
    "        results, _, _ = distill(temp, 0.1, epochs=50)\n",
    "        print(f'Acc: {results[1]} \\n Temp: {temp} \\n')\n",
    "        temp_accuracies_weight_1.append((temp, results))\n",
    "    except ValueError as e:\n",
    "        print('Temp:', temp)\n",
    "        print(e) \n",
    "temp_accuracies_weight_1 = np.asarray(temp_accuracies_weight_1)\n",
    "np.save('data/temp_accuracies_weight_1.npy', temp_accuracies_weight_1)\n",
    "\n",
    "temp_accuracies_weight_2 = []\n",
    "for temp in [1, 2, 5, 10]:\n",
    "    try:\n",
    "        results, _, _ = distill(temp, 0.5, epochs=50)\n",
    "        print(f'Acc: {results[1]} \\n Temp: {temp} \\n')\n",
    "        temp_accuracies_weight_2.append((temp, results))\n",
    "    except ValueError as e:\n",
    "        print('Temp:', temp)\n",
    "        print(e) \n",
    "temp_accuracies_weight_2 = np.asarray(temp_accuracies_weight_2)\n",
    "np.save('data/temp_accuracies_weight_2.npy', temp_accuracies_weight_2)\n",
    "\n",
    "temp_accuracies_weight_3 = []\n",
    "for temp in [1, 2, 5, 10]:\n",
    "    try:\n",
    "        results, _, _ = distill(temp, 0.9, epochs=50)\n",
    "        print(f'Acc: {results[1]} \\n Temp: {temp} \\n')\n",
    "        temp_accuracies_weight_3.append((temp, results))\n",
    "    except ValueError as e:\n",
    "        print('Temp:', temp)\n",
    "        print(e) \n",
    "temp_accuracies_weight_3 = np.asarray(temp_accuracies_weight_3)\n",
    "np.save('data/temp_accuracies_weight_3.npy', temp_accuracies_weight_3)\n",
    "\n",
    "# weight_accuracies = []\n",
    "# for weight in np.arange(0, 1.6, 0.1):\n",
    "#     try:\n",
    "#         results, _, _ = distill(10, weight, epochs=50)\n",
    "#         print(f'Acc: {results[1]} \\n Weight: {weight} \\n')\n",
    "#         weight_accuracies.append((weight, results))\n",
    "#     except ValueError as e:\n",
    "#         print('Weight:', weight)\n",
    "#         print(e) \n",
    "# weight_accuracies = np.asarray(weight_accuracies)\n",
    "# np.save('data/weight_accuracies2.npy', weight_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distillation results and plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "temp_accuracies1 = np.load('data/temp_accuracies_weight_1.npy', allow_pickle=True)\n",
    "accuracies1 = list(map(lambda x: x[1][1], temp_accuracies1))\n",
    "temp_accuracies2 = np.load('data/temp_accuracies_weight_2.npy', allow_pickle=True)\n",
    "accuracies2 = list(map(lambda x: x[1][1], temp_accuracies2))\n",
    "temp_accuracies3 = np.load('data/temp_accuracies_weight_3.npy', allow_pickle=True)\n",
    "accuracies3 = list(map(lambda x: x[1][1], temp_accuracies3))\n",
    "\n",
    "plt.plot(temp_accuracies1[:, 0], accuracies1, label='Weight = 0.1')\n",
    "plt.plot(temp_accuracies2[:, 0], accuracies2, label='Weight = 0.5')\n",
    "plt.plot(temp_accuracies3[:, 0], accuracies3, label='Weight = 0.9')\n",
    "plt.title('Temp vs Accuracy')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('weight_v_accuracy3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = list(map(lambda x: x[0], accs))\n",
    "a = list(map(lambda x: x[1], accs))\n",
    "\n",
    "plt.plot(ts, a)\n",
    "plt.title('Distillation Temp vs Accuracy')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distill single model\n",
    "res, history, model = distill(2, 0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hard loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(history.history['val_categorical_crossentropy'])+1), history.history['val_categorical_crossentropy'], label='val')\n",
    "plt.plot(range(1, len(history.history['categorical_crossentropy'])+1), history.history['categorical_crossentropy'], label='training')\n",
    "plt.title('Progression of hard logloss')\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('hard logloss');\n",
    "plt.legend()\n",
    "plt.savefig('hard_logloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot soft loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(history.history['val_soft_logloss'])+1), history.history['val_soft_logloss'], label='val')\n",
    "plt.plot(range(1, len(history.history['soft_logloss'])+1), history.history['soft_logloss'], label='training')\n",
    "plt.title('Progression of soft logloss')\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('soft logloss');\n",
    "plt.legend()\n",
    "plt.savefig('soft_logloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temp v acc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "temp_accuracies = np.load('data/temp_accuracies1.npy', allow_pickle=True)\n",
    "accuracies = list(map(lambda x: x[1][1], temp_accuracies))\n",
    "\n",
    "plt.plot(temp_accuracies[:, 0], accuracies)\n",
    "plt.title('Distillation Temp vs Accuracy')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('temp_v_accuracy1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight v acc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "weight_accuracies = np.load('data/weight_accuracies1.npy', allow_pickle=True)\n",
    "accuracies = list(map(lambda x: x[1][1], weight_accuracies))\n",
    "\n",
    "plt.plot(weight_accuracies[:, 0], accuracies)\n",
    "plt.title('Hard Loss Weight vs Accuracy')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('weight_v_accuracy1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plot of soft loss v hard loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(history.history['soft_logloss'], history.history['categorical_crossentropy'], label='training')\n",
    "plt.scatter(history.history['val_soft_logloss'], history.history['val_categorical_crossentropy'], label='val')\n",
    "plt.title('Hard Loss vs. Soft Loss')\n",
    "plt.xlabel('soft_logloss');\n",
    "plt.ylabel('logloss');\n",
    "plt.legend()\n",
    "plt.savefig('logloss_vs_softlogloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot acc curves\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train');\n",
    "plt.plot(history.history['val_accuracy'], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate_generator(val_generator_no_shuffle, 80)\n",
    "print('Errors: ', 10000 - results[1] * 10000)\n",
    "print('Accuracy:', results[1]*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, 'models/cifar10_distilled_student.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

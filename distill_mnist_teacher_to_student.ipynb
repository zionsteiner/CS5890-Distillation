{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('Knowledge-Distillation/utils/')\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "# use non standard flow_from_directory\n",
    "from image_preprocessing_ver2 import ImageDataGenerator\n",
    "# it outputs y_batch that contains onehot targets and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logits saved from teacher\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "train_logits = np.load(os.path.join(data_dir, 'mnisttrain_logits.npy'), allow_pickle=True)[()]\n",
    "val_logits = np.load(os.path.join(data_dir, 'mnistval_logits.npy'), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load mnist images and associated teacher logits into ImageDataGenerator for batch processing\n",
    "\n",
    "data_generator = ImageDataGenerator(data_format='channels_last', rescale=1/255)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'mnist\\\\train'), train_logits, target_size=(28, 28), color_mode='grayscale')\n",
    "val_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'mnist\\\\test'), val_logits, target_size=(28, 28), color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup student model\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "\n",
    "# Temps of 2.5-4 \"worked significantly better\" that other temps on networks with 30 units per layer\n",
    "temp = 4\n",
    "\n",
    "from keras import models, layers\n",
    "\n",
    "student = models.Sequential()\n",
    "student.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "student.add(layers.Dense(32, activation='relu'))\n",
    "student.add(layers.Dense(32, activation='relu'))\n",
    "student.add(layers.Dense(10))\n",
    "student.add(layers.Activation('softmax'))\n",
    "\n",
    "# Remove softmax\n",
    "student.pop()\n",
    "\n",
    "# Get student logits and class probabilities\n",
    "logits = student.layers[-1].output\n",
    "probabilities = layers.Activation('softmax')(logits)\n",
    "\n",
    "# Apply temperature to get softed probabilities\n",
    "logits_T = layers.Lambda(lambda x: x / temp)(logits)\n",
    "probabilities_T = layers.Activation('softmax')(logits_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "output = layers.concatenate([probabilities, probabilities_T])\n",
    "model = Model(student.input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss (soft targets and hard targets)\n",
    "\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras import backend as K\n",
    "\n",
    "def distillation_loss(y_true, y_pred, hard_loss_weight):\n",
    "    # \n",
    "    y_true, logits = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    y_soft = K.softmax(logits / temp)\n",
    "    \n",
    "    y_pred, y_pred_soft = y_pred[:, :10], y_pred[:, 10:]\n",
    "    \n",
    "    return hard_loss_weight * logloss(y_true, y_pred) + logloss(y_soft, y_pred_soft)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric functions\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "def top_5_accuracy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return top_k_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    y_true = y_true[:, :10]\n",
    "    y_pred = y_pred[:, :10]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def soft_logloss(y_true, y_pred):\n",
    "    print(f'y_true: {y_true.shape} y_pred: {y_pred.shape}')\n",
    "    logits = y_true[:, 10:]\n",
    "    print(f'logits: {logits.shape}')\n",
    "    y_soft = K.softmax(logits/temp)\n",
    "    print(f'y_soft: {y_soft.shape}')\n",
    "    y_pred_soft = y_pred[:, 10:]\n",
    "    print(f'y_pred_soft: {y_pred_soft.shape}')\n",
    "    return logloss(y_soft, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: (?, ?) y_pred: (?, 20)\n",
      "logits: (?, ?)\n",
      "y_soft: (?, ?)\n",
      "y_pred_soft: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "hard_loss_weight = 0.07\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.SGD(lr=1e-2, momentum=0.9, nesterov=True),\n",
    "    loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, hard_loss_weight),\n",
    "    metrics=[accuracy, top_5_accuracy, categorical_crossentropy, soft_logloss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=35,\n",
    "    steps_per_epoch=60000/32,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=100,\n",
    "    callbacks=[\n",
    "            EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.01), \n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, min_delta=0.007)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history['categorical_crossentropy'], label='train')\n",
    "plt.plot(model.history.history['val_categorical_crossentropy'], label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('logloss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['accuracy'], label='train');\n",
    "plt.plot(model.history.history['val_accuracy'], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['top_5_accuracy'], label='train');\n",
    "plt.plot(model.history.history['val_top_5_accuracy'], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('top5_accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator_no_shuffle = data_generator.flow_from_directory(\n",
    "    os.path.join(data_dir, 'mnist\\\\test'), val_logits,\n",
    "    target_size=(28, 28),\n",
    "    batch_size=64, color_mode='grayscale', shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate_generator(val_generator_no_shuffle, 80)\n",
    "print('Errors: ', 10000 - results[1] * 10000)\n",
    "print('Accuracy:', results[1]*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, 'mnist_distilled_student.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

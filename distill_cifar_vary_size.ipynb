{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "# https://github.com/Ujjwal-9/Knowledge-Distillation\n",
    "sys.path.append('Knowledge/utils/')\n",
    "import sklearn\n",
    "import os\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "import os\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "# use non standard flow_from_directory\n",
    "# it outputs y_batch that contains onehot targets and logits\n",
    "from image_preprocessing_ver2 import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logits saved from teacher\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "train_logits = np.load(os.path.join(data_dir, 'cifar10_train_logits.npy'), allow_pickle=True)[()]\n",
    "val_logits = np.load(os.path.join(data_dir, 'cifar10_val_logits.npy'), allow_pickle=True)[()]\n",
    "\n",
    "data_generator = ImageDataGenerator(data_format='channels_last', rescale=1/255)\n",
    "\n",
    "batch_size = 128\n",
    "epochs=75\n",
    "data_dir = r''\n",
    "train_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'cifar10\\\\train'), train_logits, target_size=(32, 32), color_mode='rgb', batch_size=batch_size)\n",
    "val_generator = data_generator.flow_from_directory(os.path.join(data_dir, 'cifar10\\\\test'), val_logits, target_size=(32, 32), color_mode='rgb', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and return student given dilation parameter\n",
    "\n",
    "from keras import models, layers\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "def build_student(dilation):\n",
    "    \n",
    "    if dilation > 1:\n",
    "        dilation = 1\n",
    "    if round(32 * dilation) == 0:\n",
    "        dilation = 1/32\n",
    "    \n",
    "    student = Sequential()\n",
    "    student.add(Conv2D(int(round(32*dilation)), (3, 3), input_shape=(32, 32, 3)))\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    student.add(Conv2D(int(round(64*dilation)), (3, 3)))\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    student.add(Flatten())\n",
    "    student.add(Dense(int(round(256*dilation))))\n",
    "    student.add(Activation('relu'))\n",
    "    student.add(Dense(num_classes))\n",
    "    student.add(Activation('softmax'))\n",
    "    \n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss (soft targets and hard targets)\n",
    "\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from keras import backend as K\n",
    "\n",
    "def distillation_loss(y_true, y_pred, hard_loss_weight, temp):\n",
    "    y_true, logits = y_true[:, :10], y_true[:, 10:]\n",
    "    \n",
    "    y_soft = K.softmax(logits / temp)\n",
    "    \n",
    "    y_pred, y_pred_soft = y_pred[:, :10], y_pred[:, 10:]\n",
    "    \n",
    "    return hard_loss_weight * logloss(y_true, y_pred) + logloss(y_soft, y_pred_soft)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define val data generator\n",
    "\n",
    "val_generator_no_shuffle = data_generator.flow_from_directory(\n",
    "    os.path.join(data_dir, 'cifar10\\\\test'), val_logits,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=batch_size, color_mode='rgb', shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(dilation, temp, weight):\n",
    "    \"\"\"\n",
    "    Metrics are redefined here because soft_logloss depends on non-standard param (temp).\n",
    "    model.compile wouldn't take lambdas as metrics, so this was the workaround.\n",
    "    \"\"\"\n",
    "    def accuracy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "    def top_5_accuracy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return top_k_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "    def categorical_crossentropy(y_true, y_pred):\n",
    "        y_true = y_true[:, :10]\n",
    "        y_pred = y_pred[:, :10]\n",
    "        return logloss(y_true, y_pred)\n",
    "\n",
    "    def soft_logloss(y_true, y_pred):     \n",
    "        logits = y_true[:, 10:]\n",
    "        y_soft = K.softmax(logits/temp)\n",
    "        y_pred_soft = y_pred[:, 10:]    \n",
    "        return logloss(y_soft, y_pred_soft)\n",
    "    \n",
    "    student = build_student(dilation)\n",
    "    \n",
    "    # Remove softmax\n",
    "    student.pop()\n",
    "    \n",
    "    # Get student logits and class probabilities\n",
    "    logits = student.layers[-1].output\n",
    "    probabilities = layers.Activation('softmax')(logits)\n",
    "\n",
    "    # Apply temperature to get softed probabilities\n",
    "    # Temps of 2.5-4 \"worked significantly better\" than other temps on networks with 30 units per layer\n",
    "    logits_T = layers.Lambda(lambda x: x / temp)(logits)\n",
    "    probabilities_T = layers.Activation('softmax')(logits_T)\n",
    "\n",
    "    # Define student that outputs probabilities and softed probabilities\n",
    "    output = layers.concatenate([probabilities, probabilities_T])\n",
    "    model = Model(student.input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, weight, temp),\n",
    "        metrics=[accuracy, top_5_accuracy, categorical_crossentropy, soft_logloss])\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=25,\n",
    "        steps_per_epoch=50000/batch_size,\n",
    "        verbose=0,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=25,\n",
    "        callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', patience=5, min_delta=0.005)\n",
    "            ])\n",
    "\n",
    "    results = model.evaluate_generator(val_generator_no_shuffle, 50000/batch_size)\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for non-distilled training\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training student normally\n",
    "\n",
    "def train(dilation):\n",
    "    student = build_student(dilation)\n",
    "    student.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    history = student.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=25,\n",
    "        verbose=0,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', patience=5, min_delta=0.005)\n",
    "            ])\n",
    "    results = student.evaluate(x_test, y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over dilation values, saving results\n",
    "\n",
    "dilation_accuracies = []\n",
    "reg_accuracies = []\n",
    "for dilation in np.linspace(0.125, 1, 8):\n",
    "    d_accuracy, model = distill(dilation, temp=10, weight=0.2)\n",
    "    dilation_accuracies.append((dilation, d_accuracy))\n",
    "    acc = train(dilation)\n",
    "    reg_accuracies.append((dilation, acc))\n",
    "dilation_accuracies = np.asarray(dilation_accuracies)\n",
    "reg_accuracies = np.asarray(reg_accuracies)\n",
    "np.save('data/dilation_accuracies.npy', dilation_accuracies)\n",
    "np.save('data/reg_accuracies.npy', reg_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot acc vs dilation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "d_accuracies = np.load('data/dilation_accuracies.npy', allow_pickle=True)\n",
    "dilation_x = d_accuracies[:, 0]\n",
    "dilation_y = list(map(lambda x: x[1][1], d_accuracies))\n",
    "\n",
    "reg_accuracies = np.load('data/reg_accuracies.npy', allow_pickle=True)\n",
    "reg_x = reg_accuracies[:, 0]\n",
    "reg_y = list(map(lambda x: x[1][1], reg_accuracies))\n",
    "\n",
    "plt.plot(dilation_x, dilation_y, label='Distilled')\n",
    "plt.plot(reg_x, reg_y, label='Benchmark')\n",
    "plt.title('Dilation vs Accuracy')\n",
    "plt.xlabel('Dilation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(np.linspace(0, 1, 9))\n",
    "plt.savefig('dilation_v_accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
